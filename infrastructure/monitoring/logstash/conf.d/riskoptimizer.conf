# Logstash Pipeline Configuration for RiskOptimizer Infrastructure
# /etc/logstash/conf.d/riskoptimizer.conf

# Input section - Define data sources
input {
  # Beats input for Filebeat, Metricbeat, etc.
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    ssl_verify_mode => "force_peer"
  }
  
  # Syslog input for system logs
  syslog {
    port => 5514
    ssl_enable => true
    ssl_cert => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_cacert => "/etc/logstash/certs/ca.crt"
    ssl_verify => true
  }
  
  # HTTP input for application logs
  http {
    port => 8080
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    ssl_verify_mode => "force_peer"
    codec => json
    additional_codecs => {
      "application/json" => "json"
      "text/plain" => "plain"
    }
  }
  
  # Kafka input for high-volume log streaming
  kafka {
    bootstrap_servers => "kafka-1:9092,kafka-2:9092,kafka-3:9092"
    topics => ["riskoptimizer-logs", "riskoptimizer-metrics", "riskoptimizer-audit"]
    group_id => "logstash-riskoptimizer"
    consumer_threads => 4
    decorate_events => true
    codec => json
    security_protocol => "SSL"
    ssl_truststore_location => "/etc/logstash/certs/kafka.truststore.jks"
    ssl_truststore_password => "${KAFKA_TRUSTSTORE_PASSWORD}"
    ssl_keystore_location => "/etc/logstash/certs/kafka.keystore.jks"
    ssl_keystore_password => "${KAFKA_KEYSTORE_PASSWORD}"
    ssl_key_password => "${KAFKA_KEY_PASSWORD}"
  }
  
  # JDBC input for database audit logs
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/vendor/jar/jdbc/postgresql-42.3.1.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://postgres.riskoptimizer.com:5432/riskoptimizer?ssl=true&sslmode=require"
    jdbc_user => "${POSTGRES_USER}"
    jdbc_password => "${POSTGRES_PASSWORD}"
    schedule => "*/5 * * * *"
    statement => "SELECT * FROM audit_logs WHERE created_at > :sql_last_value ORDER BY created_at ASC"
    use_column_value => true
    tracking_column => "created_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/var/lib/logstash/.logstash_jdbc_last_run"
  }
}

# Filter section - Process and transform data
filter {
  # Add common fields
  mutate {
    add_field => {
      "[@metadata][environment]" => "production"
      "[@metadata][cluster]" => "riskoptimizer"
      "[@metadata][version]" => "1.0"
    }
  }
  
  # Parse different log types based on source
  if [fields][log_type] == "application" {
    # Parse application logs
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "app"
      }
    } else {
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{DATA:logger} - %{GREEDYDATA:log_message}"
        }
      }
    }
    
    # Parse timestamp
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
    
    # Extract user information if present
    if [app][user_id] {
      mutate {
        add_field => { "user_id" => "%{[app][user_id]}" }
      }
    }
    
    # Extract session information
    if [app][session_id] {
      mutate {
        add_field => { "session_id" => "%{[app][session_id]}" }
      }
    }
    
    # Extract request information
    if [app][request_id] {
      mutate {
        add_field => { "request_id" => "%{[app][request_id]}" }
      }
    }
    
    # Classify log level
    if [level] in ["ERROR", "FATAL"] {
      mutate {
        add_tag => ["error"]
        add_field => { "severity" => "high" }
      }
    } else if [level] == "WARN" {
      mutate {
        add_tag => ["warning"]
        add_field => { "severity" => "medium" }
      }
    } else {
      mutate {
        add_field => { "severity" => "low" }
      }
    }
  }
  
  # Parse security logs
  if [fields][log_type] == "security" {
    # Parse authentication events
    if [message] =~ /authentication/ {
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[SECURITY\] %{WORD:event_type} user=%{USER:username} ip=%{IP:client_ip} result=%{WORD:auth_result}"
        }
      }
      
      mutate {
        add_tag => ["authentication"]
      }
      
      if [auth_result] == "failed" {
        mutate {
          add_tag => ["auth_failure"]
          add_field => { "severity" => "high" }
        }
      }
    }
    
    # Parse authorization events
    if [message] =~ /authorization/ {
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[SECURITY\] %{WORD:event_type} user=%{USER:username} resource=%{DATA:resource} action=%{WORD:action} result=%{WORD:authz_result}"
        }
      }
      
      mutate {
        add_tag => ["authorization"]
      }
      
      if [authz_result] == "denied" {
        mutate {
          add_tag => ["authz_failure"]
          add_field => { "severity" => "high" }
        }
      }
    }
  }
  
  # Parse audit logs
  if [fields][log_type] == "audit" {
    json {
      source => "message"
      target => "audit"
    }
    
    # Extract audit event details
    mutate {
      add_field => {
        "audit_event_type" => "%{[audit][event_type]}"
        "audit_user" => "%{[audit][user]}"
        "audit_resource" => "%{[audit][resource]}"
        "audit_action" => "%{[audit][action]}"
      }
    }
    
    # Tag compliance-related events
    if [audit_event_type] in ["data_access", "data_modification", "configuration_change"] {
      mutate {
        add_tag => ["compliance", "sox", "gdpr"]
      }
    }
    
    if [audit_event_type] in ["payment_processing", "card_data_access"] {
      mutate {
        add_tag => ["compliance", "pci_dss"]
      }
    }
  }
  
  # Parse web server logs
  if [fields][log_type] == "nginx" or [fields][log_type] == "apache" {
    grok {
      match => {
        "message" => "%{COMBINEDAPACHELOG}"
      }
    }
    
    # Parse response time if present
    if [response] {
      mutate {
        convert => { "response" => "integer" }
      }
    }
    
    # Classify HTTP status codes
    if [response] >= 500 {
      mutate {
        add_tag => ["error", "http_5xx"]
        add_field => { "severity" => "high" }
      }
    } else if [response] >= 400 {
      mutate {
        add_tag => ["client_error", "http_4xx"]
        add_field => { "severity" => "medium" }
      }
    }
    
    # Detect suspicious patterns
    if [request] =~ /(?i)(union|select|insert|delete|drop|script|javascript|<script)/ {
      mutate {
        add_tag => ["suspicious", "potential_attack"]
        add_field => { "severity" => "critical" }
      }
    }
  }
  
  # GeoIP enrichment for IP addresses
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
      database => "/usr/share/logstash/vendor/geoip/GeoLite2-City.mmdb"
    }
  }
  
  # User agent parsing
  if [agent] {
    useragent {
      source => "agent"
      target => "user_agent"
    }
  }
  
  # PII detection and masking
  if [message] =~ /\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b/ {
    mutate {
      add_tag => ["pii", "credit_card"]
    }
    
    # Mask credit card numbers
    mutate {
      gsub => [
        "message", "\b(\d{4})[\s-]?(\d{4})[\s-]?(\d{4})[\s-]?(\d{4})\b", "\1-****-****-\4"
      ]
    }
  }
  
  # SSN detection and masking
  if [message] =~ /\b\d{3}-\d{2}-\d{4}\b/ {
    mutate {
      add_tag => ["pii", "ssn"]
    }
    
    # Mask SSN
    mutate {
      gsub => [
        "message", "\b(\d{3})-(\d{2})-(\d{4})\b", "\1-**-\3"
      ]
    }
  }
  
  # Email detection and masking
  if [message] =~ /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/ {
    mutate {
      add_tag => ["pii", "email"]
    }
    
    # Mask email addresses
    mutate {
      gsub => [
        "message", "\b([A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b", "***@\2"
      ]
    }
  }
  
  # Add fingerprint for deduplication
  fingerprint {
    source => ["host", "message", "@timestamp"]
    target => "[@metadata][fingerprint]"
    method => "SHA256"
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => ["agent", "ecs", "input", "log"]
  }
}

# Output section - Send processed data to destinations
output {
  # Main Elasticsearch cluster for application logs
  if [fields][log_type] == "application" {
    elasticsearch {
      hosts => ["https://elasticsearch-1:9200", "https://elasticsearch-2:9200", "https://elasticsearch-3:9200"]
      index => "riskoptimizer-app-logs-%{+YYYY.MM.dd}"
      template_name => "riskoptimizer-app-logs"
      template_pattern => "riskoptimizer-app-logs-*"
      template => "/etc/logstash/templates/app-logs-template.json"
      template_overwrite => true
      user => "${ELASTICSEARCH_USER}"
      password => "${ELASTICSEARCH_PASSWORD}"
      ssl => true
      ssl_certificate_verification => true
      ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
      document_id => "%{[@metadata][fingerprint]}"
      retry_on_conflict => 3
    }
  }
  
  # Security logs to dedicated index
  if [fields][log_type] == "security" {
    elasticsearch {
      hosts => ["https://elasticsearch-1:9200", "https://elasticsearch-2:9200", "https://elasticsearch-3:9200"]
      index => "riskoptimizer-security-logs-%{+YYYY.MM.dd}"
      template_name => "riskoptimizer-security-logs"
      template_pattern => "riskoptimizer-security-logs-*"
      template => "/etc/logstash/templates/security-logs-template.json"
      template_overwrite => true
      user => "${ELASTICSEARCH_USER}"
      password => "${ELASTICSEARCH_PASSWORD}"
      ssl => true
      ssl_certificate_verification => true
      ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
      document_id => "%{[@metadata][fingerprint]}"
      retry_on_conflict => 3
    }
  }
  
  # Audit logs to compliance index
  if [fields][log_type] == "audit" {
    elasticsearch {
      hosts => ["https://elasticsearch-1:9200", "https://elasticsearch-2:9200", "https://elasticsearch-3:9200"]
      index => "riskoptimizer-audit-logs-%{+YYYY.MM.dd}"
      template_name => "riskoptimizer-audit-logs"
      template_pattern => "riskoptimizer-audit-logs-*"
      template => "/etc/logstash/templates/audit-logs-template.json"
      template_overwrite => true
      user => "${ELASTICSEARCH_USER}"
      password => "${ELASTICSEARCH_PASSWORD}"
      ssl => true
      ssl_certificate_verification => true
      ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
      document_id => "%{[@metadata][fingerprint]}"
      retry_on_conflict => 3
    }
  }
  
  # Critical alerts to alerting system
  if "error" in [tags] or "critical" in [tags] or [severity] == "critical" {
    http {
      url => "https://alertmanager.riskoptimizer.com/api/v1/alerts"
      http_method => "post"
      format => "json"
      headers => {
        "Content-Type" => "application/json"
        "Authorization" => "Bearer ${ALERTMANAGER_TOKEN}"
      }
      ssl_certificate_verification => true
      ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    }
  }
  
  # Backup to S3 for long-term retention
  s3 {
    access_key_id => "${AWS_ACCESS_KEY_ID}"
    secret_access_key => "${AWS_SECRET_ACCESS_KEY}"
    region => "us-west-2"
    bucket => "riskoptimizer-log-archive"
    prefix => "logs/%{+YYYY}/%{+MM}/%{+dd}/"
    time_file => 60
    codec => "json_lines"
    server_side_encryption => true
    server_side_encryption_algorithm => "AES256"
  }
  
  # Debug output for troubleshooting (remove in production)
  # stdout {
  #   codec => rubydebug
  # }
}

